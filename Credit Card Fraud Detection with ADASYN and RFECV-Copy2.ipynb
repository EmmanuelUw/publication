{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BwsJgfvMjPu2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "from sklearn.model_selection import RepeatedKFold, cross_validate, train_test_split\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import numpy\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T6paDhbqWeKF"
   },
   "outputs": [],
   "source": [
    "# Load the dataset to pandas\n",
    "dataset = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# df = read_csv('creditcard.csv', engine='python')\n",
    "\n",
    "# Create the X and y samples\n",
    "X = dataset.drop('Class', axis=1)\n",
    "y = dataset['Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "E0gF_70oZyw1"
   },
   "outputs": [],
   "source": [
    "# Apply ADASYN to the training set (sampling_strategy=0.3)\n",
    "adasyn = ADASYN(sampling_strategy=0.3, random_state=1)\n",
    "X, y = adasyn.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PDsPJQw9WljS"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_trainA, X_testA, y_trainA, y_testA = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CoIbKbG7WqaM"
   },
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "# model = DecisionTreeClassifier()\n",
    "\n",
    "# DT = DecisionTreeClassifier(random_state=0)\n",
    "# LR = LogisticRegression(random_state=0, max_iter=500)\n",
    "# KNN = KNeighborsClassifier()\n",
    "# RF = RandomForestClassifier(random_state=0)\n",
    "# LGBM = LGBMClassifier(random_state=0)\n",
    "# MLPC = MLPClassifier(random_state=0, max_iter=1500)\n",
    "# XGB = XGBClassifier(random_state=0)\n",
    "\n",
    "GNB = GaussianNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zlWw1nNpaC7J"
   },
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination with Cross-Validation (RFECV):\n",
    "\n",
    "#Overview: RFECV is an extension of RFE that selects features by recursively removing the least important ones and uses cross-validation to find the optimal number of features.\n",
    "# Example: sklearn's RFECV module combines feature elimination and cross-validation.\n",
    "# Initialize RFECV with the model and scoring metric\n",
    "\n",
    "# rfecv_DT = RFECV(estimator=DT, step=1, cv=10, scoring='accuracy')\n",
    "# rfecv_LR = RFECV(estimator=LR, step=1, cv=10, scoring='accuracy')\n",
    "# rfecv_RF = RFECV(estimator=RF, step=1, cv=10, scoring='accuracy')\n",
    "# rfecv_LGBM = RFECV(estimator=LGBM, step=1, cv=10, scoring='accuracy')\n",
    "# rfecv_MLPC = RFECV(estimator=MLPC, step=1, cv=10, scoring='accuracy')\n",
    "# rfecv_XGB = RFECV(estimator=XGB, step=1, cv=10, scoring='accuracy')\n",
    "# # rfecv_KNN = RFECV(estimator=KNN)\n",
    "\n",
    "\n",
    "rfecv_GNB = RFECV(estimator=GNB, step=1, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvwG_mdIWwUa",
    "outputId": "873198df-5379-4e43-fbd2-ef323c2d4ef7"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "when `importance_getter=='auto'`, the underlying estimator GaussianNB should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # Fit RFECV to the training data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# rfecv_DT.fit(X_trainA, y_trainA)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# rfecv_LR.fit(X_trainA, y_trainA)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# rfecv_MLPC.fit(X_trainA, y_trainA)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# rfecv_XGB.fit(X_trainA, y_trainA)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mrfecv_GNB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trainA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trainA\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:710\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    707\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    708\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[1;32m--> 710\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    715\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[0;32m    716\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:711\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    707\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    708\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[0;32m    710\u001b[0m scores \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m--> 711\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    713\u001b[0m )\n\u001b[0;32m    715\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[0;32m    716\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:37\u001b[0m, in \u001b[0;36m_rfe_single_fit\u001b[1;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[0;32m     35\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[0;32m     36\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, test, train)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscores_\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:286\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    283\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m importances \u001b[38;5;241m=\u001b[39m \u001b[43m_get_feature_importances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimportance_getter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msquare\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m ranks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(importances)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# for sparse case ranks is matrix\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py:204\u001b[0m, in \u001b[0;36m_get_feature_importances\u001b[1;34m(estimator, getter, transform_func, norm_order)\u001b[0m\n\u001b[0;32m    202\u001b[0m         getter \u001b[38;5;241m=\u001b[39m attrgetter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    205\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen `importance_getter==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`, the underlying \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`coef_` or `feature_importances_` attribute. Either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a fitted estimator to feature selector or call fit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore calling transform.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    210\u001b[0m         )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m     getter \u001b[38;5;241m=\u001b[39m attrgetter(getter)\n",
      "\u001b[1;31mValueError\u001b[0m: when `importance_getter=='auto'`, the underlying estimator GaussianNB should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform."
     ]
    }
   ],
   "source": [
    "# # Fit RFECV to the training data\n",
    "# rfecv_DT.fit(X_trainA, y_trainA)\n",
    "# rfecv_LR.fit(X_trainA, y_trainA)\n",
    "# # rfecv_KNN.fit(X_trainA, y_trainA)\n",
    "# rfecv_RF.fit(X_trainA, y_trainA)\n",
    "# rfecv_LGBM.fit(X_trainA, y_trainA)\n",
    "# rfecv_MLPC.fit(X_trainA, y_trainA)\n",
    "# rfecv_XGB.fit(X_trainA, y_trainA)\n",
    "\n",
    "rfecv_GNB.fit(X_trainA, y_trainA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5u3zKC9FWzxf"
   },
   "outputs": [],
   "source": [
    "# # Print the optimal number of features\n",
    "# print(\"Optimal Number of Features, DT:\", rfecv_DT.n_features_)\n",
    "# print(\"Optimal Number of Features, LR:\", rfecv_LR.n_features_)\n",
    "# # print(\"Optimal Number of Features, KNN:\", rfecv_KNN.n_features_)\n",
    "# print(\"Optimal Number of Features, RF:\", rfecv_RF.n_features_)\n",
    "# print(\"Optimal Number of Features, LGBM:\", rfecv_LGBM.n_features_)\n",
    "# # print(\"Optimal Number of Features, MLPC:\", rfecv_MLPC.n_features_)\n",
    "# # print(\"Optimal Number of Features, XGB:\", rfecv_XGB.n_features_)\n",
    "\n",
    "print(\"Optimal Number of Features, GNB:\", rfecv_GNB.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00SB28amW3wD"
   },
   "outputs": [],
   "source": [
    "# # Transform the training and testing data to keep only the selected features\n",
    "# X_trainA_selected_DT = rfecv_DT.transform(X_trainA)\n",
    "# X_trainA_selected_LR = rfecv_LR.transform(X_trainA)\n",
    "# # X_trainA_selected_KNN = rfecv_KNN.transform(X_trainA)\n",
    "# X_trainA_selected_RF = rfecv_RF.transform(X_trainA)\n",
    "# X_trainA_selected_LGBM = rfecv_LGBM.transform(X_trainA)\n",
    "# # X_trainA_selected_MLPC = rfecv_MLPC.transform(X_trainA)\n",
    "# # X_trainA_selected_XGB = rfecv_XGB.transform(X_trainA)\n",
    "\n",
    "X_trainA_selected_GNB = rfecv_GNB.transform(X_trainA)\n",
    "\n",
    "\n",
    "# X_testA_selected_DT = rfecv_DT.transform(X_testA)\n",
    "# X_testA_selected_LR = rfecv_LR.transform(X_testA)\n",
    "# # X_testA_selected_KNN = rfecv_KNN.transform(X_testA)\n",
    "# X_testA_selected_RF = rfecv_RF.transform(X_testA)\n",
    "# X_testA_selected_LGBM = rfecv_LGBM.transform(X_testA)\n",
    "# # X_testA_selected_MLPC = rfecv_MLPC.transform(X_testA)\n",
    "# # X_testA_selected_XGB = rfecv_XGB.transform(X_testA)\n",
    "\n",
    "X_testA_selected_GNB = rfecv_GNB.transform(X_testA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EAq0hEiW7mN"
   },
   "outputs": [],
   "source": [
    "# Train a model on the selected features\n",
    "# model.fit(X_trainA_selected, y_trainA)\n",
    "\n",
    "# history_DT = DT.fit(X_trainA_selected_DT, y_trainA)\n",
    "# history_LR = LR.fit(X_trainA_selected_LR, y_trainA)\n",
    "# # history_KNN = KNN.fit(X_trainA_selected_KNN, y_trainA)\n",
    "# history_RF = RF.fit(X_trainA_selected_RF, y_trainA)\n",
    "# history_LGBM = LGBM.fit(X_trainA_selected_LGBM, y_trainA)\n",
    "# # history_XGB = XGB.fit(X_trainA_selected_XGB, y_trainA)\n",
    "# # history_MLPC = MLPC.fit(X_trainA_selected_MLPC, y_trainA)\n",
    "\n",
    "history_GNB = GNB.fit(X_trainA_selected_GNB, y_trainA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x63geQ2CW9ux"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "# y_pred = model.predict(X_testA_selected)\n",
    "\n",
    "# # Making predictions on test data\n",
    "# y_pred_DT = DT.predict(X_testA_selected_DT)\n",
    "# y_pred_LR = LR.predict(X_testA_selected_LR)\n",
    "# # y_pred_KNN = KNN.predict(X_testA_selected_KNN)\n",
    "# y_pred_RF = RF.predict(X_testA_selected_RF)\n",
    "# y_pred_LGBM = LGBM.predict(X_testA_selected_LGBM)\n",
    "# # y_pred_XGB = XGB.predict(X_testA_selected_XGB)\n",
    "# # y_pred_MLPC = MLPC.predict(X_testA_selected_MLPC)\n",
    "\n",
    "y_pred_GNB = GNB.predict(X_testA_selected_GNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4v9XSYXkdbB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3870580f"
   },
   "outputs": [],
   "source": [
    "# confusion_matrix = metrics.confusion_matrix(y_testA, y_pred_DT)\n",
    "# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "# cm_display.plot()\n",
    "# cm_display.ax_.set_title('DT with ADASYN-RFECV')\n",
    "# # plt.show()\n",
    "# plt.savefig('DT with ADASYN-RFECV.eps')\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(y_testA, y_pred_LR)\n",
    "# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "# cm_display.plot()\n",
    "# cm_display.ax_.set_title('LR with ADASYN-RFECV')\n",
    "# # plt.show()\n",
    "# plt.savefig('LR with ADASYN-RFECV.eps')\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(y_testA, y_pred_KNN)\n",
    "# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "# cm_display.plot()\n",
    "# cm_display.ax_.set_title('KNN with ADASYN-RFECV')\n",
    "# # plt.show()\n",
    "# plt.savefig('KNN with ADASYN-RFECV.eps')\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(y_testA, y_pred_RF)\n",
    "# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "# cm_display.plot()\n",
    "# cm_display.ax_.set_title('RF with ADASYN-RFECV')\n",
    "# # plt.show()\n",
    "# plt.savefig('RF with ADASYN-RFECV.eps')\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(y_testA, y_pred_LGBM)\n",
    "# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "# cm_display.plot()\n",
    "# cm_display.ax_.set_title('LGBM with ADASYN-RFECV')\n",
    "# # plt.show()\n",
    "# plt.savefig('LGBM with ADASYN-RFECV.eps')\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(y_testA, y_pred_XGB)\n",
    "# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "# cm_display.plot()\n",
    "# cm_display.ax_.set_title('XGB with ADASYN-RFECV')\n",
    "# # plt.show()\n",
    "# plt.savefig('XGB with ADASYN-RFECV.eps')\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(y_testA, y_pred_MLPC)\n",
    "# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "# cm_display.plot()\n",
    "# cm_display.ax_.set_title('MLPC with ADASYN-RFECV')\n",
    "# # plt.show()\n",
    "# plt.savefig('MLPC with ADASYN-RFECV.eps')\n",
    "\n",
    "\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_testA, y_pred_GNB)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "cm_display.plot()\n",
    "cm_display.ax_.set_title('GNB with ADASYN-RFECV')\n",
    "# plt.show()\n",
    "plt.savefig('GNB with ADASYN-RFECV.eps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvIXflHKkuo3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d12aa11a"
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "# y_pred_DT = cross_val_predict(DT, X_trainA, y_trainA, cv=cv)\n",
    "# y_pred_LR = cross_val_predict(LR, X_trainA, y_trainA, cv=cv)\n",
    "# # y_pred_KNN = cross_val_predict(KNN, X_trainA, y_trainA, cv=cv)\n",
    "# y_pred_RF = cross_val_predict(RF, X_trainA, y_trainA, cv=cv)\n",
    "# y_pred_LGBM = cross_val_predict(LGBM, X_trainA, y_trainA, cv=cv)\n",
    "# # y_pred_XGB = cross_val_predict(XGB, X_trainA, y_trainA, cv=cv)\n",
    "# # y_pred_MLPC = cross_val_predict(MLPC, X_trainA, y_trainA, cv=cv)\n",
    "\n",
    "y_pred_GNB = cross_val_predict(GNB, X_trainA, y_trainA, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nz0-23Szk2gu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1f9aa9b"
   },
   "outputs": [],
   "source": [
    "# # performance metrics\n",
    "# print('...............')\n",
    "# print('DT:')\n",
    "# # accuracy: (tp + tn) / (p + n)\n",
    "# accuracy = accuracy_score(y_trainA, y_pred_DT)\n",
    "# print('Accuracy: %.4f' % accuracy)\n",
    "# # precision tp / (tp + fp)\n",
    "# precision = precision_score(y_trainA, y_pred_DT)\n",
    "# print('Precision: %.4f' % precision)\n",
    "# # recall: tp / (tp + fn)\n",
    "# recall = recall_score(y_trainA, y_pred_DT)\n",
    "# print('Recall: %.4f' % recall)\n",
    "# # f1: 2 tp / (2 tp + fp + fn)\n",
    "# f1 = f1_score(y_trainA, y_pred_DT)\n",
    "# print('F1 score: %.4f' % f1)\n",
    "# MCC = matthews_corrcoef(y_trainA, y_pred_DT)\n",
    "# print('MCC: %.4f' % (MCC))\n",
    "\n",
    "# # performance metrics\n",
    "# print('...............')\n",
    "# print('LR:')\n",
    "# # accuracy: (tp + tn) / (p + n)\n",
    "# accuracy = accuracy_score(y_trainA, y_pred_LR)\n",
    "# print('Accuracy: %.4f' % accuracy)\n",
    "# # precision tp / (tp + fp)\n",
    "# precision = precision_score(y_trainA, y_pred_LR)\n",
    "# print('Precision: %.4f' % precision)\n",
    "# # recall: tp / (tp + fn)\n",
    "# recall = recall_score(y_trainA, y_pred_LR)\n",
    "# print('Recall: %.4f' % recall)\n",
    "# # f1: 2 tp / (2 tp + fp + fn)\n",
    "# f1 = f1_score(y_trainA, y_pred_LR)\n",
    "# print('F1 score: %.4f' % f1)\n",
    "# MCC = matthews_corrcoef(y_trainA, y_pred_LR)\n",
    "# print('MCC: %.4f' % (MCC))\n",
    "\n",
    "# # performance metrics\n",
    "# print('...............')\n",
    "# print('KNN:')\n",
    "# # accuracy: (tp + tn) / (p + n)\n",
    "# accuracy = accuracy_score(y_trainA, y_pred_KNN)\n",
    "# print('Accuracy: %.4f' % accuracy)\n",
    "# # precision tp / (tp + fp)\n",
    "# precision = precision_score(y_trainA, y_pred_KNN)\n",
    "# print('Precision: %.4f' % precision)\n",
    "# # recall: tp / (tp + fn)\n",
    "# recall = recall_score(y_trainA, y_pred_KNN)\n",
    "# print('Recall: %.4f' % recall)\n",
    "# # f1: 2 tp / (2 tp + fp + fn)\n",
    "# f1 = f1_score(y_trainA, y_pred_KNN)\n",
    "# print('F1 score: %.4f' % f1)\n",
    "# MCC = matthews_corrcoef(y_trainA, y_pred_KNN)\n",
    "# print('MCC: %.4f' % (MCC))\n",
    "\n",
    "# # performance metrics\n",
    "# print('...............')\n",
    "# print('RF:')\n",
    "# # accuracy: (tp + tn) / (p + n)\n",
    "# accuracy = accuracy_score(y_trainA, y_pred_RF)\n",
    "# print('Accuracy: %.4f' % accuracy)\n",
    "# # precision tp / (tp + fp)\n",
    "# precision = precision_score(y_trainA, y_pred_RF)\n",
    "# print('Precision: %.4f' % precision)\n",
    "# # recall: tp / (tp + fn)\n",
    "# recall = recall_score(y_trainA, y_pred_RF)\n",
    "# print('Recall: %.4f' % recall)\n",
    "# # f1: 2 tp / (2 tp + fp + fn)\n",
    "# f1 = f1_score(y_trainA, y_pred_RF)\n",
    "# print('F1 score: %.4f' % f1)\n",
    "# MCC = matthews_corrcoef(y_trainA, y_pred_RF)\n",
    "# print('MCC: %.4f' % (MCC))\n",
    "\n",
    "# # performance metrics\n",
    "# print('...............')\n",
    "# print('LGBM:')\n",
    "# # accuracy: (tp + tn) / (p + n)\n",
    "# accuracy = accuracy_score(y_trainA, y_pred_LGBM)\n",
    "# print('Accuracy: %.4f' % accuracy)\n",
    "# # precision tp / (tp + fp)\n",
    "# precision = precision_score(y_trainA, y_pred_LGBM)\n",
    "# print('Precision: %.4f' % precision)\n",
    "# # recall: tp / (tp + fn)\n",
    "# recall = recall_score(y_trainA, y_pred_LGBM)\n",
    "# print('Recall: %.4f' % recall)\n",
    "# # f1: 2 tp / (2 tp + fp + fn)\n",
    "# f1 = f1_score(y_trainA, y_pred_LGBM)\n",
    "# print('F1 score: %.4f' % f1)\n",
    "# MCC = matthews_corrcoef(y_trainA, y_pred_LGBM)\n",
    "# print('MCC: %.4f' % (MCC))\n",
    "\n",
    "# # performance metrics\n",
    "# print('...............')\n",
    "# print('XGB:')\n",
    "# # accuracy: (tp + tn) / (p + n)\n",
    "# accuracy = accuracy_score(y_trainA, y_pred_XGB)\n",
    "# print('Accuracy: %.4f' % accuracy)\n",
    "# # precision tp / (tp + fp)\n",
    "# precision = precision_score(y_trainA, y_pred_XGB)\n",
    "# print('Precision: %.4f' % precision)\n",
    "# # recall: tp / (tp + fn)\n",
    "# recall = recall_score(y_trainA, y_pred_XGB)\n",
    "# print('Recall: %.4f' % recall)\n",
    "# # f1: 2 tp / (2 tp + fp + fn)\n",
    "# f1 = f1_score(y_trainA, y_pred_XGB)\n",
    "# print('F1 score: %.4f' % f1)\n",
    "# MCC = matthews_corrcoef(y_trainA, y_pred_XGB)\n",
    "# print('MCC: %.4f' % (MCC))\n",
    "\n",
    "# # performance metrics\n",
    "# print('...............')\n",
    "# print('MLPC:')\n",
    "# # accuracy: (tp + tn) / (p + n)\n",
    "# accuracy = accuracy_score(y_trainA, y_pred_MLPC)\n",
    "# print('Accuracy: %.4f' % accuracy)\n",
    "# # precision tp / (tp + fp)\n",
    "# precision = precision_score(y_trainA, y_pred_MLPC)\n",
    "# print('Precision: %.4f' % precision)\n",
    "# # recall: tp / (tp + fn)\n",
    "# recall = recall_score(y_trainA, y_pred_MLPC)\n",
    "# print('Recall: %.4f' % recall)\n",
    "# # f1: 2 tp / (2 tp + fp + fn)\n",
    "# f1 = f1_score(y_trainA, y_pred_MLPC)\n",
    "# print('F1 score: %.4f' % f1)\n",
    "# MCC = matthews_corrcoef(y_trainA, y_pred_MLPC)\n",
    "# print('MCC: %.4f' % (MCC))\n",
    "\n",
    "\n",
    "# performance metrics\n",
    "print('...............')\n",
    "print('GNB:')\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_trainA, y_pred_GNB)\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_trainA, y_pred_GNB)\n",
    "print('Precision: %.4f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_trainA, y_pred_GNB)\n",
    "print('Recall: %.4f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_trainA, y_pred_GNB)\n",
    "print('F1 score: %.4f' % f1)\n",
    "MCC = matthews_corrcoef(y_trainA, y_pred_GNB)\n",
    "print('MCC: %.4f' % (MCC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzyMmtlFW_pI"
   },
   "outputs": [],
   "source": [
    "# # Evaluate the model's performance\n",
    "# accuracy = accuracy_score(y_testA, y_pred)\n",
    "# print(\"Model Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IF0jneMZXCDt"
   },
   "outputs": [],
   "source": [
    "# # Plot the feature selection process\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "# plt.xlabel(\"Number of Features Selected\")\n",
    "# plt.ylabel(\"Cross-Validation Score (Accuracy)\")\n",
    "# plt.title(\"RFECV: Cross-Validation Score vs. Number of Features Selected\")\n",
    "# plt.show()\n",
    "\n",
    "# # Plot the feature selection process\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(1, len(rfecv.cv_results_) + 1), rfecv.cv_results_)\n",
    "# plt.xlabel(\"Number of Features Selected\")\n",
    "# plt.ylabel(\"Cross-Validation Score (Accuracy)\")\n",
    "# plt.title(\"RFECV: Cross-Validation Score vs. Number of Features Selected\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
